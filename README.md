# Comparative Analysis of Deep Learning Architectures and Optimizer Performance

This repository contains implementations and analysis of multiple deep learning models, including Deep Neural Networks (DNN), Convolutional Neural Networks (CNN), Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM). It also includes a comparative study of optimizer performance in training these models.

## Overview
Deep learning has transformed the landscape of machine learning by enabling powerful predictive models. This project explores:
- Implementation of various deep learning architectures.
- Performance analysis of optimizers like SGD, Adam, and RMSprop.
- Insights into model behavior on accuracy, loss, and training time.

## Features
- **Model Implementations:** DNN, CNN, GRU, LSTM
- **Optimizer Comparisons:** Evaluation of optimizer efficiency and convergence rates.
- **Performance Metrics:** Accuracy, training time, loss curves.
- **Visualization:** Comprehensive plots for result interpretation.

## Files in the Repository
- **DNN_assignment_2a_group10.ipynb:** Implementation and evaluation of DNN.
- **2b_CNN_group10.ipynb:** Implementation of CNN and analysis.
- **2b_GRU_group10.ipynb:** GRU implementation and performance metrics.
- **2b_LSTM_group10.ipynb:** LSTM implementation and evaluation.
- **2b_Comparative_Analysis_of_Optimizer_Performance_group10.ipynb:** Optimizer performance analysis.

## Results
Key findings:
- DNNs perform better for structured data but struggle with temporal dependencies.
- LSTMs and GRUs outperform CNNs in sequence modeling tasks.
- Adam optimizer provides faster convergence compared to SGD and RMSprop.

## Usage
1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/DeepLearning_GroupProject.git
   
